{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beast/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/beast/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/beast/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from smq_quantizer import W8A8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"codegen-350M-mono\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from functools import partial\n",
    "# import copy\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def quantize_weight_per_channel_absmax(w, n_bits=8):\n",
    "#     # w: (out_features, in_features) : Linear\n",
    "#     # w: (out_channels, in_channels, kernel_size, kernel_size) : Conv2d\n",
    "#     if w.dim() == 4:\n",
    "#         w_copied = copy.deepcopy(w)\n",
    "#         w = w.view(w.shape[0], -1)\n",
    "#     scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "#     q_max = 2 ** (n_bits - 1) - 1\n",
    "#     scales.clamp_(min=1e-5).div_(q_max)\n",
    "#     if w.dim() == 4:\n",
    "#         scales = scales.view(w_copied.shape[0], 1, 1, 1)\n",
    "#         w = w_copied  # Copy back the original weights\n",
    "#     w.div_(scales).round_().mul_(scales)\n",
    "#     return w\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def quantize_weight_per_tensor_absmax(w, n_bits=8):\n",
    "#     # w: (out_features, in_features)\n",
    "#     scales = w.abs().max()\n",
    "#     q_max = 2 ** (n_bits - 1) - 1\n",
    "#     scales.clamp_(min=1e-5).div_(q_max)\n",
    "#     w.div_(scales).round_().mul_(scales)\n",
    "#     return w\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "#     t_shape = t.shape\n",
    "#     t = t.view(-1, t_shape[-1])\n",
    "#     scales = t.abs().max(dim=-1, keepdim=True)[0]\n",
    "#     q_max = 2 ** (n_bits - 1) - 1\n",
    "#     scales.clamp_(min=1e-5).div_(q_max)\n",
    "#     t.div_(scales).round_().mul_(scales)\n",
    "#     return t.view(t_shape)  # Reshape back to original\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "#     t_shape = t.shape\n",
    "#     t = t.view(-1, t_shape[-1])\n",
    "#     scales = t.abs().max()\n",
    "#     q_max = 2 ** (n_bits - 1) - 1\n",
    "#     scales.clamp_(min=1e-5).div_(q_max)\n",
    "#     t.div_(scales).round_().mul_(scales)\n",
    "#     return t.view(t_shape)  # Reshape back to original\n",
    "\n",
    "# class W8A8(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         in_features,  # C_in\n",
    "#         out_features,  # C_out\n",
    "#         kernel_size=None,\n",
    "#         stride=None,\n",
    "#         padding=None,\n",
    "#         dilation=None,\n",
    "#         groups=None,\n",
    "#         bias=True,\n",
    "#         act_quant=\"per_token\",\n",
    "#         quantize_output=False,\n",
    "#         cnn=False,\n",
    "#         dtype=None\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.in_features = in_features\n",
    "#         self.out_features = out_features\n",
    "        \n",
    "#         self.cnn = cnn\n",
    "#         self.ype = dtype\n",
    "#         self.dtype = torch.float16 if self.ype is not None else dtype \n",
    "#         if cnn:\n",
    "#             self.kernel_size = kernel_size\n",
    "#             self.stride = stride\n",
    "#             self.padding = padding\n",
    "#             self.dilation = dilation\n",
    "#             self.groups = groups\n",
    "#             self.weight_shape = (self.out_features, self.in_features, self.kernel_size, self.kernel_size)\n",
    "#         else:\n",
    "#             self.weight_shape = (self.out_features, self.in_features)\n",
    "            \n",
    "#         self.register_buffer(\n",
    "#             \"weight\",\n",
    "#             torch.randn(\n",
    "#                 self.weight_shape,\n",
    "#                 dtype=self.dtype,\n",
    "#                 requires_grad=False,\n",
    "#             ),\n",
    "#         )\n",
    "#         if bias:\n",
    "#             self.register_buffer(\n",
    "#                 \"bias\",\n",
    "#                 torch.zeros(\n",
    "#                     (1, self.out_features), dtype=self.dtype, requires_grad=False\n",
    "#                 ),\n",
    "#             )\n",
    "#         else:\n",
    "#             self.register_buffer(\"bias\", None)\n",
    "\n",
    "#         if act_quant == \"per_token\":\n",
    "#             self.act_quant_name = \"per_token\"\n",
    "#             self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "#         elif act_quant == \"per_tensor\":\n",
    "#             self.act_quant_name = \"per_tensor\"\n",
    "#             self.act_quant = partial(quantize_activation_per_tensor_absmax, n_bits=8)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "#         if quantize_output:\n",
    "#             self.output_quant_name = self.act_quant_name\n",
    "#             self.output_quant = self.act_quant\n",
    "#         else:\n",
    "#             self.output_quant_name = \"None\"\n",
    "#             self.output_quant = lambda x: x\n",
    "\n",
    "#     def to(self, *args, **kwargs):\n",
    "#         super(W8A8, self).to(*args, **kwargs)\n",
    "#         self.weight = self.weight.to(*args, **kwargs)\n",
    "#         if self.bias is not None:\n",
    "#             self.bias = self.bias.to(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def forward(self, x):\n",
    "#         q_x = self.act_quant(x)\n",
    "#         if self.weight.dim() == 4:\n",
    "#             y = torch.nn.functional.conv2d(q_x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "#         else:\n",
    "#             y = torch.nn.functional.linear(q_x, self.weight, self.bias)\n",
    "#         q_y = self.output_quant(y)\n",
    "#         return q_y\n",
    "\n",
    "#     @staticmethod\n",
    "#     def from_float(\n",
    "#         module, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False\n",
    "#     ):\n",
    "#         # Weight per_channel/per_tensor quantization; Activation per_token/per_tensor quantization\n",
    "#         if isinstance(module, torch.nn.Linear):\n",
    "#             new_module = W8A8(\n",
    "#                 module.in_features,\n",
    "#                 module.out_features,\n",
    "#                 bias=module.bias is not None,\n",
    "#                 act_quant=act_quant,\n",
    "#                 quantize_output=quantize_output,\n",
    "#                 dtype=module.weight.data.dtype\n",
    "#             )\n",
    "#         elif isinstance(module, torch.nn.Conv2d):\n",
    "#             new_module = W8A8(\n",
    "#                 module.in_channels,\n",
    "#                 module.out_channels,\n",
    "#                 module.kernel_size,\n",
    "#                 module.stride,\n",
    "#                 module.padding,\n",
    "#                 module.dilation,\n",
    "#                 module.groups,\n",
    "#                 bias=module.bias is not None,\n",
    "#                 act_quant=act_quant,\n",
    "#                 quantize_output=quantize_output,\n",
    "#                 cnn=True,\n",
    "#                 dtype=module.weight.data.dtype\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported module type\")\n",
    "\n",
    "#         if weight_quant == \"per_channel\":\n",
    "#             new_module.weight = quantize_weight_per_channel_absmax(\n",
    "#                 module.weight, n_bits=8\n",
    "#             )  # use 8-bit integer for weight\n",
    "#         elif weight_quant == \"per_tensor\":\n",
    "#             new_module.weight = quantize_weight_per_tensor_absmax(\n",
    "#                 module.weight, n_bits=8\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "\n",
    "#         new_module.weight_quant_name = weight_quant\n",
    "\n",
    "#         if module.bias is not None:\n",
    "#             new_module.bias = module.bias\n",
    "\n",
    "#         return new_module\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         if self.cnn:\n",
    "#             return f\"W8A8Conv2d-smq({self.in_features}, {self.out_features}, kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}, dilation={self.dilation}, groups={self.groups}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name})\"\n",
    "#         else:\n",
    "#             return f\"W8A8Linear-smq({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_target_and_quantize_smq(module, \n",
    "                               target_class, module_name_to_exclude):\n",
    "  \n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not \\\n",
    "        any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "\n",
    "            new_module = target_class.from_float(child, quantize_output=False) #,  weight_quant=\"per_token\", act_quant=\"per_token\")\n",
    "           \n",
    "            setattr(module, name, new_module)\n",
    "\n",
    "            # getattr(module, name).quantize(old_weight)\n",
    "            \n",
    "            if old_bias is not None:\n",
    "              getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            # Recursively call the function for nested modules\n",
    "            replace_linear_with_target_and_quantize_smq(child, \n",
    "                     target_class, module_name_to_exclude)\n",
    "\n",
    "# del model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                    torch_dtype=torch.bfloat16, \n",
    "                                             low_cpu_mem_usage=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after:\n",
      "\n",
      " CodeGenForCausalLM(\n",
      "  (transformer): CodeGenModel(\n",
      "    (wte): Embedding(51200, 1024)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-19): 20 x CodeGenBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CodeGenAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (qkv_proj): W8A8Linear-smq(1024, 3072, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (out_proj): W8A8Linear-smq(1024, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "        )\n",
      "        (mlp): CodeGenMLP(\n",
      "          (fc_in): W8A8Linear-smq(1024, 4096, bias=True, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (fc_out): W8A8Linear-smq(4096, 1024, bias=True, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
      ")\n",
      "def hello_world():\n",
      "    print(\"Hello World\")\n",
      "\n",
      "hello_world()\n",
      "\n",
      "# íŒŒ\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target_and_quantize_smq(model, W8A8, [\"lm_head\"])\n",
    "\n",
    "print(\"Model after:\\n\\n\", pipe.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe(\"def hello_world():\", max_new_tokens=20, \n",
    "           do_sample=False)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
